{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import cv2\n",
    "from imutils.video import VideoStream\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Locally read in data and make our training and testing set\n",
    "DATA_DIR = \"rockpaperscissors\"\n",
    "CATEGORIES = [\"paper\", \"rock\", \"scissors\"]\n",
    "IMG_WIDTH = 150\n",
    "IMG_HEIGHT = 100\n",
    "BATCH_SIZE = 100\n",
    "    \n",
    "def load_data(data_dir, categories, img_width, img_height):\n",
    "    X = []\n",
    "    y = []\n",
    "    index = -1\n",
    "    for category in categories:\n",
    "        index += 1\n",
    "        one_hot = np.zeros(len(categories))  # to encode the class as a one hot vector\n",
    "        one_hot[index] = 1\n",
    "        path = os.path.join(data_dir, category)\n",
    "        \n",
    "        for img in os.listdir(path):  # get all images in the path\n",
    "            img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            img_arr = cv2.resize(img_arr, (img_width, img_height))\n",
    "            img_arr = np.asarray(img_arr)\n",
    "            X.append(img_arr)\n",
    "            y.append(one_hot)\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "X, y = load_data(DATA_DIR, CATEGORIES, IMG_WIDTH, IMG_HEIGHT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1750, 150, 100, 1)\n",
      "1750 train samples\n",
      "438 test samples\n"
     ]
    }
   ],
   "source": [
    "# Divide our data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, shuffle=True)\n",
    "\n",
    "# Normalize our data\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "# reshape the data into a 4D tensor - (sample_number, x_img_size, y_img_size, num_channels)\n",
    "# because we are using greyscale, we only have a single channel - RGB colour images would have 3\n",
    "X_train = X_train.reshape(X_train.shape[0], IMG_WIDTH, IMG_HEIGHT, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], IMG_WIDTH, IMG_HEIGHT, 1)\n",
    "input_shape = (IMG_WIDTH, IMG_HEIGHT, 1)\n",
    "\n",
    "# convert the data to the right type\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('x_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the CNN\n",
    "classifier = tf.keras.models.Sequential()\n",
    "\n",
    "# Create model\n",
    "\n",
    "# 3x 2d convolution layers\n",
    "# Non-linearity (RELU) - replace all negative pixel values in feature map with zero\n",
    "classifier.add(tf.keras.layers.Conv2D(32, (3,3), input_shape=(IMG_WIDTH, IMG_HEIGHT, 1), activation='relu')) \n",
    "classifier.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "classifier.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "classifier.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten 3d model into 1d\n",
    "classifier.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# feature vectors\n",
    "classifier.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dropout(0.5))\n",
    "classifier.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "classifier.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=\"rmsprop\",\n",
    "                   metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1750 samples, validate on 438 samples\n",
      "Epoch 1/13\n",
      "1750/1750 [==============================] - 12s 7ms/sample - loss: 1.0991 - accuracy: 0.3343 - val_loss: 1.0995 - val_accuracy: 0.3265\n",
      "Epoch 2/13\n",
      "1750/1750 [==============================] - 11s 7ms/sample - loss: 1.0987 - accuracy: 0.3531 - val_loss: 1.0992 - val_accuracy: 0.3265\n",
      "Epoch 3/13\n",
      "1750/1750 [==============================] - 11s 6ms/sample - loss: 1.0976 - accuracy: 0.3526 - val_loss: 1.0996 - val_accuracy: 0.3265\n",
      "Epoch 4/13\n",
      "1750/1750 [==============================] - 12s 7ms/sample - loss: 1.0991 - accuracy: 0.3463 - val_loss: 1.0991 - val_accuracy: 0.3265\n",
      "Epoch 5/13\n",
      "1750/1750 [==============================] - 11s 6ms/sample - loss: 1.0985 - accuracy: 0.3417 - val_loss: 1.0993 - val_accuracy: 0.3265\n",
      "Epoch 6/13\n",
      "1750/1750 [==============================] - 12s 7ms/sample - loss: 1.0981 - accuracy: 0.3451 - val_loss: 1.0997 - val_accuracy: 0.3265\n",
      "Epoch 7/13\n",
      "1750/1750 [==============================] - 11s 7ms/sample - loss: 1.0985 - accuracy: 0.3451 - val_loss: 1.0996 - val_accuracy: 0.3265\n",
      "Epoch 8/13\n",
      "1750/1750 [==============================] - 11s 7ms/sample - loss: 1.0982 - accuracy: 0.3497 - val_loss: 1.0998 - val_accuracy: 0.3265\n",
      "Epoch 9/13\n",
      "1750/1750 [==============================] - 12s 7ms/sample - loss: 1.0986 - accuracy: 0.3423 - val_loss: 1.0997 - val_accuracy: 0.3265\n",
      "Epoch 10/13\n",
      "1750/1750 [==============================] - 11s 7ms/sample - loss: 1.0981 - accuracy: 0.3457 - val_loss: 1.0999 - val_accuracy: 0.3265\n",
      "Epoch 11/13\n",
      "1750/1750 [==============================] - 11s 6ms/sample - loss: 1.0983 - accuracy: 0.3434 - val_loss: 1.0997 - val_accuracy: 0.3265\n",
      "Epoch 12/13\n",
      "1750/1750 [==============================] - 11s 6ms/sample - loss: 1.0985 - accuracy: 0.3469 - val_loss: 1.0996 - val_accuracy: 0.3265\n",
      "Epoch 13/13\n",
      "1750/1750 [==============================] - 11s 6ms/sample - loss: 1.0984 - accuracy: 0.3469 - val_loss: 1.0996 - val_accuracy: 0.3265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x154b47208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run and train model\n",
    "classifier.fit(np.array(X_train), np.array(y_train),\n",
    "               batch_size=BATCH_SIZE,\n",
    "               epochs=13, \n",
    "               verbose=1,\n",
    "               validation_data=(np.array(X_test), np.array(y_test))\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test model\n",
    "score = classifier.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "image \"pyimage2\" doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-418c4472bb35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mGUI_live_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-418c4472bb35>\u001b[0m in \u001b[0;36mGUI_live_detection\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_background_opencv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_idletasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-418c4472bb35>\u001b[0m in \u001b[0;36mupdate_background_opencv\u001b[0;34m(self, cv_image)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageTk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhotoImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36mconfigure\u001b[0;34m(self, cnf, **kw)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mallowed\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \"\"\"\n\u001b[0;32m-> 1485\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'configure'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m_configure\u001b[0;34m(self, cmd, cnf, kw)\u001b[0m\n\u001b[1;32m   1474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getconfigure1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1477\u001b[0m     \u001b[0;31m# These used to be defined in Widget:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTclError\u001b[0m: image \"pyimage2\" doesn't exist"
     ]
    }
   ],
   "source": [
    "def detect_and_display(model, video_source):\n",
    "    # Get frame from video soruce\n",
    "    frame = video_source.read()\n",
    "    \n",
    "    # Convert image for model\n",
    "    small = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    gray = cv2.cvtColor(small, cv2.COLOR_RGB2GRAY)\n",
    "    # Reshape for input to NN\n",
    "    img_arr = gray.reshape(1, IMG_WIDTH, IMG_HEIGHT, 1)\n",
    "    # Cast to float to handle error\n",
    "    img_arr = tf.cast(img_arr, tf.float32)\n",
    "    # Save colour image to show to user\n",
    "    colour = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    prediction = classifier.predict(img_arr)\n",
    "    # Convert prediction from one hot to category\n",
    "    index = tf.argmax(prediction[0], axis=0)\n",
    "    prediction = CATEGORIES[index]\n",
    "    \n",
    "    cv2.putText(frame, prediction, (20, 50), cv2.FONT_HERSHEY_SIMPLEX,3, (0, 255, 0), 2)\n",
    "    return frame\n",
    "    \n",
    "    \n",
    "def live_detection(model):\n",
    "    video_source = VideoStream(src=0).start()\n",
    "    \n",
    "    while True:\n",
    "        frame = detect_and_display(model, video_source)\n",
    "        cv2.imshow(\"Face Liveness Detector\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    video_source.stop()\n",
    "    \n",
    "class Application(tk.Frame):\n",
    "    def play_game(self):\n",
    "        print(\"Let's play a game...\")\n",
    "        self.update_background(\"s\")\n",
    "    \n",
    "    def exit_game(self):\n",
    "        self.open = False\n",
    "\n",
    "    def createWidgets(self):\n",
    "        self.IMAGE = tk.Label(self)\n",
    "        self.IMAGE.image = None\n",
    "        self.IMAGE.place(relx=0, rely=0, relwidth=1, relheight=1)\n",
    "\n",
    "        self.BUTTONS = tk.Frame(self)\n",
    "        self.BUTTONS.pack(side=\"top\")\n",
    "\n",
    "        self.QUIT = tk.Button(self.BUTTONS)\n",
    "        self.QUIT[\"text\"] = \"QUIT\"\n",
    "        self.QUIT[\"fg\"]   = \"red\"\n",
    "        self.QUIT[\"height\"]   = \"3\"\n",
    "        self.QUIT[\"width\"]   = \"15\"\n",
    "        self.QUIT[\"command\"] =  self.exit_game\n",
    "        self.QUIT.pack(side=\"left\")\n",
    "\n",
    "        self.PLAY = tk.Button(self.BUTTONS)\n",
    "        self.PLAY[\"text\"] = \"PLAY\",\n",
    "        self.PLAY[\"height\"]   = \"3\"\n",
    "        self.PLAY[\"width\"]   = \"15\"\n",
    "        self.PLAY[\"command\"] = self.play_game\n",
    "        self.PLAY.pack(side=\"left\")\n",
    "\n",
    "    def update_background_opencv(self, cv_image):\n",
    "        image = ImageTk.PhotoImage(Image.fromarray(cv_image))\n",
    "        self.IMAGE.image = image\n",
    "        self.IMAGE.configure(image=image)\n",
    "\n",
    "    def __init__(self, master=None):\n",
    "        tk.Frame.__init__(self, master)\n",
    "        self.open = True\n",
    "        self.pack(side=\"top\", fill=\"both\", expand=1)\n",
    "        self.createWidgets()\n",
    "\n",
    "def GUI_live_detection(model):\n",
    "    root = tk.Tk()\n",
    "    root.geometry(\"1080x720+300+100\")\n",
    "    app = Application(master=root)\n",
    "    \n",
    "    video_source = VideoStream(src=0).start()\n",
    "\n",
    "    while app.open:\n",
    "        image = detect_and_display(model, video_source)\n",
    "        image = cv2.flip(image, 1)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        app.update_background_opencv(image)\n",
    "        app.update_idletasks()\n",
    "        app.update()\n",
    "    root.destroy()\n",
    "    \n",
    "GUI_live_detection(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
